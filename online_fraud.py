# -*- coding: utf-8 -*-
"""Online Fraud.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yGJcDkiFvuta7bQEcNYf2ac9b0jQSLsP

# Data Cleaning and Wrangling
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import numpy as np

path = '/content/drive/MyDrive/Data/online_payments.csv'
payments = pd.read_csv(path)
payments.head()

payments.isnull().sum()

payments.info()

payments.describe()

"""Scaling won't be needed here because most of the columns are associated with amounts"""

# Checking the values in the step column
payments['step'].value_counts()

payments['step'].hist(bins=30)
plt.title('Distribution of the values in the step column');

payments['isFraud'].value_counts()

payments['isFlaggedFraud'].value_counts()

"""It is clear here to see that the amount of flagged fraud transactions is significantly less than the number of actual fraud transactions"""

# Dropping columns 
payments.drop(columns=['step','nameDest', 'nameOrig'], inplace=True)

payments.rename(columns={'oldbalanceOrg':'old_balance_orig', 
                         'newbalanceOrig':'new_balance_orig', 
                         'oldbalanceDest':'old_balance_dest',
                         'newbalanceDest':'new_balance_dest',
                         'isFraud':'is_fraud',
                         'isFlaggedFraud':'is_flagged_fraud'}, inplace=True)

payments['type'].value_counts()

payments['type'].value_counts()

"""We can observe that cashout and payment take up more than 60% of all online transactions. While debit is the most insignificant"""

# Checking the correlation between the columns
plt.figure(figsize=(15,8))
sns.heatmap(payments.corr());

"""From this, we can see a few things:

1.The old_balance_orig and the new_balance_orig columns have a very high correlation.

2.The old_balance_dest and the new_balance_dest columns also have a very high correlation.

3.The amount column has some correlation, about 0.4 to 0.5, with the old_balance_dest and new_balane_dest columns.

**Analysing the is_fraud column:**

Creating a fraud variable to see the chhahracteristics of the fraudulent transactions in the dataset
"""

payments["is_fraud"] = payments["is_fraud"].map({0: "No Fraud", 1: "Fraud"})

payments['is_flagged_fraud'] = payments['is_flagged_fraud'].map({0: "No", 1: "Yes"})

payments.head()

fraud = payments[payments['is_fraud'] == 'Fraud']
fraud

fraud['type'].value_counts()

sns.countplot(data=fraud, x='type', color='blue')
plt.xlabel('Payment Type')
plt.title('Distribution of fraudulent transactions');

"""It is clear to see that only the transfer and the cashout transactions have samples that were fraudulent, with cashout hahving a slightly higher number of fraudulent transactions.

# Model Building

Mapping the type column so it can be used in training of the model
"""

payments["type"] = payments["type"].map({"CASH_OUT": 1, "PAYMENT": 2, 
                                 "CASH_IN": 3, "TRANSFER": 4,
                                 "DEBIT": 5})

payments.head()

x = np.array(payments[['type','amount', 'old_balance_orig', 'new_balance_orig']])
y = np.array(payments['is_fraud'])

y.shape

"""Randomly splitting the variables into training and testing data using scikit learn's train test split."""

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)
x_train.shape

y_train.shape

"""## Decision Tree Classifier

Using the decision tree classifier to perdict fraudulent transactions just providing the type of transaction, the amount and your old and new account balances.
"""

from sklearn.tree import DecisionTreeClassifier

dt_model = DecisionTreeClassifier()

dt_model.fit(x_train, y_train)

"""### Testing Accuracy"""

from sklearn.metrics import confusion_matrix, classification_report

dt_pred = dt_model.predict(x_test)

print(confusion_matrix(y_test, dt_pred))

print(classification_report(y_test, dt_pred))

features = np.array([[1, 72782.82, 72782.82, 0]])
print(dt_model.predict(features))

"""## Logistic Regression Model

Using logistic regression to predict the same and checking which model's precision is higher.
"""

from sklearn.linear_model import LogisticRegression
lr_model = LogisticRegression()

lr_model.fit(x_train, y_train)

"""### Testing Accuracy"""

lr_pred = lr_model.predict(x_test)

print(confusion_matrix(y_test, lr_pred))

print(classification_report(y_test, lr_pred))

features = np.array([[4, 181.0, 181.0, 0]])
print(lr_model.predict(features))

"""Since the Decision Tree Clasifier has a precision of 80% and the logistic regression has one of 81%, I would build my model on the logistic regression.

# Saving the Model

importing the pickle package to save the trained model. This model would be used in deploying the web application.
"""

import pickle

filename = 'trained_lr_model.csv'
pickle.dump(lr_model, open(filename, 'wb'))

"""## Loading the saved model"""

loaded_model = pickle.load(open('trained_lr_model.csv', 'rb'))

